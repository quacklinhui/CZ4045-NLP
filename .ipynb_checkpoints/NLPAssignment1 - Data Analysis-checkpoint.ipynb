{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "r6uxrEY2vsbO"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from random import randrange\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c anaconda nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ev8bYME-DsZH"
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "with open('reviewSamples20.json') as f:\n",
    "    data = json.loads(\"[\" + \n",
    "        f.read().replace(\"}\\n{\", \"},\\n{\") + \n",
    "    \"]\")\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eH_l11yBNvAA",
    "outputId": "d5cb94d2-8273-4c3d-f5d3-54e5219c8c70"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random review and business_id\n",
    "m = randrange(len(data))\n",
    "business_id_1 = data[m]['business_id']\n",
    "business_id_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28prWWxpDzgv"
   },
   "outputs": [],
   "source": [
    "#Extract all the reviews and form a small dataset\n",
    "preStem = []\n",
    "for j in range(len(data)):\n",
    "    if(data[j]['business_id'] == business_id_1):\n",
    "        text = str(data[j]['text']).lower()\n",
    "        word = word_tokenize(text)\n",
    "        preStem.extend(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_preStem = [w for w in preStem if not w in string.punctuation]\n",
    "print(filtered_preStem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "stems = [ps.stem(w) for w in filtered_preStem]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showing the word frequency distribution\n",
    "freq_dist = FreqDist(filtered_preStem)\n",
    "freq_dist = dict(freq_dist)\n",
    "freq_dist = pd.Series(freq_dist)\n",
    "freq_dist.sort_values(ascending=False, inplace=True)\n",
    "freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_freq_dist = FreqDist(stems)\n",
    "stemmed_freq_dist = dict(stemmed_freq_dist)\n",
    "stemmed_freq_dist = pd.Series(stemmed_freq_dist)\n",
    "stemmed_freq_dist.sort_values(ascending=False, inplace=True)\n",
    "stemmed_freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#notice that this one does not work well on 've \n",
    "fig, ax = plt.subplots(figsize=(10,30))\n",
    "sns.barplot(x=freq_dist.values, y=freq_dist.index, ax=ax)\n",
    "plt.title(\"Word Frequency Distribution - Before Stemming (Not Log-scale)\")\n",
    "plt.show()\n",
    "plt.savefig('wordFreqBeforeStemming.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,30))\n",
    "sns.barplot(x=stemmed_freq_dist.values, y=stemmed_freq_dist.index, ax=ax)\n",
    "plt.title(\"Word Frequency Distribution - After Stemming (Not Log-scale)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "4NlfUoWMMCSr",
    "outputId": "89052f73-4e0a-4451-b696-a6f1b5170731"
   },
   "outputs": [],
   "source": [
    "counts = Counter(preStem).items()\n",
    "labels, values = zip(*counts)\n",
    "indexes = np.arange(len(labels))\n",
    "width = 1\n",
    "plt.bar(indexes,values,width)\n",
    "plt.xticks(indexes ,labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "dvkI9lqaNI2V",
    "outputId": "fe532d5d-78f3-4be6-fbcc-42bd4058ce41"
   },
   "outputs": [],
   "source": [
    "counts = Counter(stems).items()\n",
    "labels, values = zip(*counts)\n",
    "indexes = np.arange(len(labels))\n",
    "width = 1\n",
    "plt.bar(indexes,values,width)\n",
    "plt.xticks(indexes ,labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a second random business\n",
    "i = randrange(len(data))\n",
    "business_id_2 = data[i]['business_id']\n",
    "business_id_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gGxYGqbDOAd9",
    "outputId": "adff3573-ad88-42a4-a175-5152e0fc811e"
   },
   "outputs": [],
   "source": [
    "preStem_2 = []\n",
    "for j in range(len(data)):\n",
    "    if(data[j]['business_id'] == business_id_2):\n",
    "        text = str(data[j]['text']).lower()\n",
    "        words = word_tokenize(text)\n",
    "        preStem_2.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_preStem_2 = [w for w in preStem_2 if not w in string.punctuation]\n",
    "# Stemming\n",
    "stems_2 = [ps.stem(w) for w in filtered_preStem_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist_2 = FreqDist(filtered_preStem_2)\n",
    "freq_dist_2 = dict(freq_dist_2)\n",
    "freq_dist_2 = pd.Series(freq_dist_2)\n",
    "freq_dist_2.sort_values(ascending=False, inplace=True)\n",
    "freq_dist_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_freq_dist_2 = FreqDist(stems_2)\n",
    "stemmed_freq_dist_2 = dict(stemmed_freq_dist_2)\n",
    "stemmed_freq_dist_2 = pd.Series(stemmed_freq_dist_2)\n",
    "stemmed_freq_dist_2.sort_values(ascending=False, inplace=True)\n",
    "stemmed_freq_dist_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,30))\n",
    "sns.barplot(x=freq_dist_2.values, y=freq_dist_2.index, ax=ax)\n",
    "plt.title(\"Word Frequency Distribution - Before Stemming (Not Log-scale)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,30))\n",
    "sns.barplot(x=stemmed_freq_dist_2.values, y=stemmed_freq_dist_2.index, ax=ax)\n",
    "plt.title(\"Word Frequency Distribution - Before Stemming (Not Log-scale)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "cZplfdfmOF4c",
    "outputId": "3878adc1-45a7-4337-88ac-a6f4f6de0083"
   },
   "outputs": [],
   "source": [
    "counts = Counter(preStem_2).most_common(10)\n",
    "labels, values = zip(*counts)\n",
    "indexes = np.arange(len(labels))\n",
    "width = 1\n",
    "plt.bar(indexes,values,width)\n",
    "plt.xticks(indexes ,labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "ino-Ma9kOI5J",
    "outputId": "a7aea637-59af-4ec4-ca3e-09b35e5c6ac2"
   },
   "outputs": [],
   "source": [
    "counts = Counter(stems_2).most_common(10)\n",
    "labels, values = zip(*counts)\n",
    "indexes = np.arange(len(labels))\n",
    "width = 1\n",
    "plt.bar(indexes,values,width)\n",
    "plt.xticks(indexes ,labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Stopwords - Ideally we want to remove stopwords before stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_EhApglVOKvO"
   },
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business id 1\n",
    "# Remove stop words\n",
    "filtered_preStem = [w for w in filtered_preStem if not w in sw]\n",
    "stems = [ps.stem(w) for w in filtered_preStem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq_dist = FreqDist(filtered_preStem).most_common(10)\n",
    "freq_dist = dict(freq_dist)\n",
    "freq_dist = pd.Series(freq_dist)\n",
    "freq_dist.sort_values(ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_freq_dist = FreqDist(stems).most_common(10)\n",
    "stemmed_freq_dist = dict(stemmed_freq_dist)\n",
    "stemmed_freq_dist = pd.Series(stemmed_freq_dist)\n",
    "stemmed_freq_dist.sort_values(ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "sns.barplot(x=freq_dist.index, y=freq_dist.values, ax=ax)\n",
    "plt.title(\"Top-10 Most Frequent Words - Before Stemming (Not Log-scale)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "sns.barplot(x=stemmed_freq_dist.index, y=stemmed_freq_dist.values, ax=ax)\n",
    "plt.title(\"Top-10 Most Frequent Words - After Stemming (Not Log-scale)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for business id 2\n",
    "# Remove stop words\n",
    "filtered_preStem_2 = [w for w in filtered_preStem_2 if not w in sw]\n",
    "stems_2 = [ps.stem(w) for w in filtered_preStem_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist_2 = FreqDist(filtered_preStem_2).most_common(10)\n",
    "freq_dist_2 = dict(freq_dist_2)\n",
    "freq_dist_2 = pd.Series(freq_dist_2)\n",
    "freq_dist_2.sort_values(ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_freq_dist_2 = FreqDist(stems_2).most_common(10)\n",
    "stemmed_freq_dist_2 = dict(stemmed_freq_dist_2)\n",
    "stemmed_freq_dist_2 = pd.Series(stemmed_freq_dist_2)\n",
    "stemmed_freq_dist_2.sort_values(ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "sns.barplot(x=freq_dist_2.index, y=freq_dist_2.values, ax=ax)\n",
    "plt.title(\"Top-10 Most Frequent Words - Before Stemming (Not Log-scale)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "sns.barplot(x=stemmed_freq_dist_2.index, y=stemmed_freq_dist_2.values, ax=ax)\n",
    "plt.title(\"Top-10 Most Frequent Words - Before Stemming (Not Log-scale)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for review in data:\n",
    "    dummy_list = nltk.tokenize.sent_tokenize(review['text'])\n",
    "    sentences.extend(dummy_list)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "...     (r'.*ing$', 'VBG'),                # gerunds\n",
    "...     (r'.*ed$', 'VBD'),                 # simple past\n",
    "...     (r'.*es$', 'VBZ'),                 # 3rd singular present\n",
    "...     (r'.*ould$', 'MD'),                # modals\n",
    "...     (r'.*\\'s$', 'NN$'),                # possessive nouns\n",
    "...     (r'.*s$', 'NNS'),                  # plural nouns\n",
    "...     (r'^-?[0-9]+(\\.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "...     (r'.*', 'NN')                      # nouns (default)\n",
    "... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "for i in range(5):\n",
    "    # Pick a random sentence\n",
    "    sentence = random.choice(sentences)\n",
    "    \n",
    "    # Tokenise chosen sentence\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    \n",
    "    # NLTK pos tagger\n",
    "    sample_POS = nltk.pos_tag(word_tokens)\n",
    "    # REGEXP tagging\n",
    "    regrex_output = regexp_tagger.tag(word_tokens)\n",
    "    \n",
    "    # SPACY POS tagging\n",
    "    spacy_tags = []\n",
    "    for word in word_tokens:\n",
    "        spacy_tags.append(nlp(word)[0].tag_)\n",
    "    \n",
    "    # Print output\n",
    "    print(\"Sentence \" + str(i+1) + \": \")\n",
    "    print(f\"{'Word':{15}} {'NLTK POS Tag':{15}} {'REGREX POS TAG':{15}} {'spaCy POS TAG':{15}}\")\n",
    "    for i in range(len(word_tokens)):\n",
    "        print(f'{word_tokens[i]:{15}} {sample_POS[i][1]:{15}} {regrex_output[i][1]:{15}} {spacy_tags[i]:{15}}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Style\n",
    "\n",
    "\n",
    "### Articles Chosen: \n",
    "\n",
    "*Channel News Asia*\n",
    "> 1. https://www.channelnewsasia.com/cnainsider/school-counselling-challenge-safeguard-student-mental-health-2081496\n",
    "2. https://www.channelnewsasia.com/commentary/china-ant-group-alibaba-didi-crackdown-tech-ipo-2149091\n",
    "\n",
    "*Stackoverflow*\n",
    ">1. https://stackoverflow.com/questions/69235547/flutter-app-and-woocommerce-integration-add-to-cart-function\n",
    "2. https://stackoverflow.com/questions/65907012/debugging-error-org-apache-axis2-axisfault-connection-or-outbound-has-closed-t\n",
    "\n",
    "*HardwareZone*\n",
    ">1. https://forums.hardwarezone.com.sg/threads/beginners-bible-to-bodybuilding-supplements-fat-loss-newbies-pls-read.3419226/\n",
    "2. https://forums.hardwarezone.com.sg/threads/need-advice-no-internet-for-desktop-on-singtel-fibre.6075480/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns (number of non capitalised proper nouns, total number of proper nouns)\n",
    "def checkProperNounCapitalised(text, nlp):\n",
    "    count = 0\n",
    "    not_capitalised = 0\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"PROPN\":\n",
    "            count += 1\n",
    "            if not token.text[0].isupper():\n",
    "                not_capitalised += 1\n",
    "    return (not_capitalised, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkFirstWordCapitalised(text):\n",
    "    punctuations = '''!()-[]{};:'\"“”‘’\\,<>./?@#$%^&*_~'''\n",
    "    \n",
    "    count = 0\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        no_punct = \"\"\n",
    "        for char in sentence:\n",
    "           if char not in punctuations:\n",
    "               no_punct = no_punct + char\n",
    "        if no_punct[0].isupper():\n",
    "            count += 1\n",
    "    return (count, len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "def q3_stem(text):\n",
    "    overflow_prestem = []\n",
    "    word = word_tokenize(text)\n",
    "    overflow_prestem.extend(word)\n",
    "    filtered_overflow_preStem = [w for w in overflow_prestem if not w in string.punctuation]\n",
    "    return filtered_overflow_preStem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Language checking function (CNA)\n",
    "import language_tool_python\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "def check_language_error(data):\n",
    "    totalerrors = 0\n",
    "    wordcount = 0\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        test_sentence = data[i].text\n",
    "        wordcount += len(test_sentence)\n",
    "        matches = tool.check(test_sentence)\n",
    "    #     for i in range(len(matches)):\n",
    "    #         print(matches[i])\n",
    "        totalerrors = len(matches) + totalerrors\n",
    "    print(\"Number of errors = \",totalerrors)\n",
    "    print(\"Number of words = \",wordcount)\n",
    "    # Percentage of language error among all words\n",
    "    language_error_percentage = (totalerrors/wordcount)*100\n",
    "    print(\"Percentage of language error = \",language_error_percentage)\n",
    "    \n",
    "## Language checking function (StackOverflow)\n",
    "import language_tool_python\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "def check_language_error_so(data):\n",
    "    totalerrors = 0\n",
    "    wordcount = 0\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        test_sentence = data[i]\n",
    "        wordcount += len(test_sentence)\n",
    "        matches = tool.check(test_sentence)\n",
    "    #     for i in range(len(matches)):\n",
    "    #         print(matches[i])\n",
    "        totalerrors = len(matches) + totalerrors\n",
    "    print(\"Number of errors = \",totalerrors)\n",
    "    print(\"Number of words = \",wordcount)\n",
    "    # Percentage of language error among all words\n",
    "    language_error_percentage = (totalerrors/wordcount)*100\n",
    "    print(\"Percentage of language error = \",language_error_percentage)\n",
    "\n",
    "## Language checking function (Hardware Zone)\n",
    "import language_tool_python\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "def check_language_error_hwz(data):\n",
    "    totalerrors = 0\n",
    "    wordcount = 0\n",
    "\n",
    "    test_sentence = data\n",
    "    wordcount += len(test_sentence)\n",
    "    matches = tool.check(test_sentence)\n",
    "    #     for i in range(len(matches)):\n",
    "    #         print(matches[i])\n",
    "    totalerrors = len(matches) + totalerrors\n",
    "    print(\"Number of errors = \",totalerrors)\n",
    "    print(\"Number of words = \",wordcount)\n",
    "    # Percentage of language error among all words\n",
    "    language_error_percentage = (totalerrors/wordcount)*100\n",
    "    print(\"Percentage of language error = \",language_error_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# scrape post from CNA\n",
    "def scrape_text_CNA(website):\n",
    "    site = requests.get(website)\n",
    "    \n",
    "    content = BeautifulSoup(site.content, 'html.parser')\n",
    "    post = content.select('div.text-long p')\n",
    "    return post\n",
    "\n",
    "# scrape post from Stackoverflow\n",
    "def scrape_text_stackoverflow(website):\n",
    "    site = requests.get(website)\n",
    "        \n",
    "    content = BeautifulSoup(site.content, 'html.parser')\n",
    "    questions = content.select('div.s-prose.js-post-body p')\n",
    "    return questions\n",
    "\n",
    "# scrape code from Stackoverflow\n",
    "def scrape_code_stackoverflow(website):\n",
    "    site = requests.get(website)\n",
    "    content = BeautifulSoup(site.content, 'html.parser')\n",
    "    code = content.select('div.s-prose.js-post-body pre code')\n",
    "    return code\n",
    "\n",
    "# scrape post from HardwareZone\n",
    "def scrape_text_hardwarezone(website):\n",
    "    site = requests.get(website)\n",
    "    content = BeautifulSoup(site.content, 'html.parser')\n",
    "    for script in content([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "    wrap = content.select('article.message-body.js-selectToQuote div.bbWrapper')\n",
    "    return wrap[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CNA #1\n",
    "\n",
    "Article Name: ‘With school counsellors, it’s really hit-or-miss’: Behind the challenge of safeguarding student mental health\n",
    "\n",
    "Website: https://www.channelnewsasia.com/cnainsider/school-counselling-challenge-safeguard-student-mental-health-2081496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# need to load article before running function\n",
    "cna1 = scrape_text_CNA('https://www.channelnewsasia.com/cnainsider/school-counselling-challenge-safeguard-student-mental-health-2081496')\n",
    "for i in range(len(cna1)):\n",
    "    print(cna1[i].text)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_language_error(cna1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "caps_cna_1 = []\n",
    "total_count_cna1 = 0\n",
    "total_numSentences_cna1 = 0\n",
    "\n",
    "for i in range(len(cna1)):\n",
    "    caps_cna_1 += [cna1[i].text]\n",
    "\n",
    "for i in range(len(caps_cna_1)):\n",
    "    (count, numSentences) = checkFirstWordCapitalised(caps_cna_1[i])\n",
    "    total_count_cna1 += count\n",
    "    total_numSentences_cna1 += numSentences\n",
    "\n",
    "print(\"Total count of capitalised first words: \", total_count_cna1)\n",
    "print(\"Total number of sentences: \", total_numSentences_cna1)\n",
    "print(\"Percentage: \", (total_count_cna1/total_numSentences_cna1) * 100)\n",
    "\n",
    "# analysis: first sentence - caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_cna1 = []\n",
    "pronoun_count_cna1 = 0\n",
    "total_pronoun_cna1 = 0\n",
    "\n",
    "for i in range(len(cna1)):\n",
    "    pronoun_cna1 += [cna1[i].text]\n",
    "\n",
    "for i in range(len(pronoun_cna1)):\n",
    "    (count, numSentences) = checkProperNounCapitalised(pronoun_cna1[i], nlp)\n",
    "    pronoun_count_cna1 += count\n",
    "    total_pronoun_cna1 += numSentences\n",
    "\n",
    "print(\"Total count of non-capitalised capitalised pronouns: \", pronoun_count_cna1)\n",
    "print(\"Total number of pronouns: \", total_pronoun_cna1)\n",
    "print(\"Percentage: \", (pronoun_count_cna1/total_pronoun_cna1) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CNA #2\n",
    "\n",
    "Article Name: Commentary: China's smackdown on corporate giants may be a start of national rebuilding\n",
    "\n",
    "Website: https://www.channelnewsasia.com/commentary/china-ant-group-alibaba-didi-crackdown-tech-ipo-2149091"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to load article before running function\n",
    "cna2 = scrape_text_CNA('https://www.channelnewsasia.com/commentary/china-ant-group-alibaba-didi-crackdown-tech-ipo-2149091')\n",
    "for i in range(len(cna2)):\n",
    "    print(cna2[i].text)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_language_error(cna2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_cna_2 = []\n",
    "total_count_cna2 = 0\n",
    "total_numSentences_cna2 = 0\n",
    "\n",
    "for i in range(len(cna2)):\n",
    "    caps_cna_2 += [cna2[i].text]\n",
    "\n",
    "for i in range(len(caps_cna_2)):\n",
    "    (count_cna2, numSentences_cna2) = checkFirstWordCapitalised(caps_cna_2[i])\n",
    "    total_count_cna2 += count_cna2\n",
    "    total_numSentences_cna2 += numSentences_cna2\n",
    "\n",
    "print(\"Total count of capitalised first words: \", total_count_cna2)\n",
    "print(\"Total number of sentences: \", total_numSentences_cna2)\n",
    "print(\"Percentage: \", (total_count_cna2/total_numSentences_cna2) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_cna2 = []\n",
    "pronoun_count_cna2 = 0\n",
    "total_pronoun_cna2 = 0\n",
    "\n",
    "for i in range(len(cna2)):\n",
    "    pronoun_cna2 += [cna2[i].text]\n",
    "\n",
    "for i in range(len(pronoun_cna2)):\n",
    "    (count_cna2, numSentences_cna2) = checkProperNounCapitalised(pronoun_cna2[i], nlp)\n",
    "    pronoun_count_cna2 += count_cna2\n",
    "    total_pronoun_cna2 += numSentences_cna2\n",
    "\n",
    "print(\"Total count of non-capitalised capitalised pronouns: \", pronoun_count_cna2)\n",
    "print(\"Total number of pronouns: \", total_pronoun_cna2)\n",
    "print(\"Percentage: \", (pronoun_count_cna2/total_pronoun_cna2) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### StackOverflow #1\n",
    "\n",
    "Post Name: flutter app and woocommerce integration (add to cart function)\n",
    "\n",
    "Website: https://stackoverflow.com/questions/69235547/flutter-app-and-woocommerce-integration-add-to-cart-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping of Post Text\n",
    "soText_1 = scrape_text_stackoverflow('https://stackoverflow.com/questions/69235547/flutter-app-and-woocommerce-integration-add-to-cart-function')\n",
    "for i in range(len(soText_1)):\n",
    "    print(soText_1[i].text)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_language_error(soText_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_so_1 = []\n",
    "total_count_so1 = 0\n",
    "total_numSentences_so1 = 0\n",
    "\n",
    "for i in range(len(soText_1)):\n",
    "    caps_so_1 += [soText_1[i].text]\n",
    "\n",
    "for i in range(len(caps_so_1)):\n",
    "    (count_so1, numSentences_so1) = checkFirstWordCapitalised(caps_so_1[i])\n",
    "    total_count_so1 += count_so1\n",
    "    total_numSentences_so1 += numSentences_so1\n",
    "\n",
    "print(\"Total count of capitalised first words: \", total_count_so1)\n",
    "print(\"Total number of sentences: \", total_numSentences_so1)\n",
    "print(\"Percentage: \", (total_count_so1/total_numSentences_so1) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_so1 = []\n",
    "pronoun_count_so1 = 0\n",
    "total_pronoun_so1 = 0\n",
    "\n",
    "for i in range(len(soText_1)):\n",
    "    pronoun_so1 += [soText_1[i].text]\n",
    "\n",
    "for i in range(len(pronoun_so1)):\n",
    "    (count_so1, numSentences_so1) = checkProperNounCapitalised(pronoun_so1[i], nlp)\n",
    "    pronoun_count_so1 += count_so1\n",
    "    total_pronoun_so1 += numSentences_so1\n",
    "\n",
    "print(\"Total count of non-capitalised capitalised pronouns: \", pronoun_count_so1)\n",
    "print(\"Total number of pronouns: \", total_pronoun_so1)\n",
    "print(\"Percentage: \", (pronoun_count_so1/total_pronoun_so1) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping of Whole Code\n",
    "soCodeWhole_1 = scrape_code_stackoverflow('https://stackoverflow.com/questions/69235547/flutter-app-and-woocommerce-integration-add-to-cart-function')\n",
    "for i in range(len(soCodeWhole_1)):\n",
    "    print(soCodeWhole_1[i].text)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting stem\n",
    "for i in range(len(soCodeWhole_1)):\n",
    "    codeStem_1 = q3_stem(soCodeWhole_1[i].text)\n",
    "    print(codeStem_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_language_error(soCodeWhole_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scraping of Post Code Comments\n",
    "import re\n",
    "soCode_1 = scrape_code_stackoverflow('https://stackoverflow.com/questions/69235547/flutter-app-and-woocommerce-integration-add-to-cart-function')\n",
    "comments_1 = []\n",
    "for i in range(len(soCode_1)):\n",
    "    pattern = re.compile('(?:/\\*(.*?)\\*/)|(?://(.*?)\\n)',re.S)\n",
    "    comment = pattern.findall(soCode_1[i].text)\n",
    "    for x in range(len(comment)):\n",
    "        for y in range(len(comment[0])):\n",
    "            star_cleaned = re.sub(r'[*]', '', comment[x][y])\n",
    "            space_cleaned = re.sub(r'[\\n ]+', ' ', star_cleaned)\n",
    "            if space_cleaned != \"\":\n",
    "                comments_1.append(space_cleaned)\n",
    "for c in range(len(comments_1)):\n",
    "    print(comments_1[c])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_language_error_so(comments_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### StackOverflow #2\n",
    "\n",
    "Post Name: Debugging error org.apache.axis2.AxisFault: Connection or outbound has closed, the trustAnchors parameter must be non-empty\n",
    "\n",
    "Website: https://stackoverflow.com/questions/65907012/debugging-error-org-apache-axis2-axisfault-connection-or-outbound-has-closed-t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Post Text\n",
    "soText_2 = scrape_text_stackoverflow('https://stackoverflow.com/questions/65907012/debugging-error-org-apache-axis2-axisfault-connection-or-outbound-has-closed-t')\n",
    "for i in range(len(soText_2)):\n",
    "    print(soText_2[i].text)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_language_error(soText_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_so_2 = []\n",
    "total_count_so2 = 0\n",
    "total_numSentences_so2 = 0\n",
    "\n",
    "for i in range(len(soText_2)):\n",
    "    caps_so_2 += [soText_2[i].text]\n",
    "\n",
    "for i in range(len(caps_so_2)):\n",
    "    (count_so2, numSentences_so2) = checkFirstWordCapitalised(caps_so_2[i])\n",
    "    total_count_so2 += count_so2\n",
    "    total_numSentences_so2 += numSentences_so2\n",
    "\n",
    "print(\"Total count of capitalised first words: \", total_count_so2)\n",
    "print(\"Total number of sentences: \", total_numSentences_so2)\n",
    "print(\"Percentage: \", (total_count_so2/total_numSentences_so2) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_so2 = []\n",
    "pronoun_count_so2 = 0\n",
    "total_pronoun_so2 = 0\n",
    "\n",
    "for i in range(len(soText_2)):\n",
    "    pronoun_so2 += [soText_2[i].text]\n",
    "\n",
    "for i in range(len(pronoun_so2)):\n",
    "    (count_so2, numSentences_so2) = checkProperNounCapitalised(pronoun_so2[i], nlp)\n",
    "    pronoun_count_so2 += count_so2\n",
    "    total_pronoun_so2 += numSentences_so2\n",
    "\n",
    "print(\"Total count of non-capitalised capitalised pronouns: \", pronoun_count_so2)\n",
    "print(\"Total number of pronouns: \", total_pronoun_so2)\n",
    "print(\"Percentage: \", (pronoun_count_so2/total_pronoun_so2) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping of Whole Code\n",
    "soCodeWhole_2 = scrape_code_stackoverflow('https://stackoverflow.com/questions/65907012/debugging-error-org-apache-axis2-axisfault-connection-or-outbound-has-closed-t')\n",
    "for i in range(len(soCodeWhole_2)):\n",
    "    print(soCodeWhole_2[i].text)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting stem\n",
    "codeStem_2_final = []\n",
    "for i in range(len(soCodeWhole_2)):\n",
    "    codeStem_2 = q3_stem(soCodeWhole_2[i].text)\n",
    "    codeStem_2_final += codeStem_2\n",
    "    \n",
    "codeStem_2_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_language_error(soCodeWhole_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Scraping of Post Code Comments\n",
    "soCode_2 = scrape_code_stackoverflow('https://stackoverflow.com/questions/65907012/debugging-error-org-apache-axis2-axisfault-connection-or-outbound-has-closed-t')\n",
    "comments_2 = []\n",
    "for i in range(len(soCode_2)):\n",
    "    pattern = re.compile('(?:/\\*(.*?)\\*/)|(?://(.*?)\\n)',re.S)\n",
    "    comment = pattern.findall(soCode_2[i].text)\n",
    "    for x in range(len(comment)):\n",
    "        for y in range(len(comment[0])):\n",
    "            star_cleaned = re.sub(r'[*]', '', comment[x][y])\n",
    "            space_cleaned = re.sub(r'[\\n ]+', ' ', star_cleaned)\n",
    "            if space_cleaned != \"\":\n",
    "                comments_2.append(space_cleaned)\n",
    "for c in range(len(comments_2)):\n",
    "    print(comments_2[c])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_language_error_so(comments_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HardwareZone #1\n",
    "\n",
    "Post Name: Beginner's Bible To Bodybuilding/Supplements/Fat-Loss ( Newbies Pls Read!!!)\n",
    "\n",
    "Website: https://forums.hardwarezone.com.sg/threads/beginners-bible-to-bodybuilding-supplements-fat-loss-newbies-pls-read.3419226/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping data from HardwareZone\n",
    "hwz_1 = scrape_text_hardwarezone('https://forums.hardwarezone.com.sg/threads/beginners-bible-to-bodybuilding-supplements-fat-loss-newbies-pls-read.3419226/')\n",
    "print(hwz_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_language_error_hwz(hwz_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count_hwz1 = 0\n",
    "total_numSentences_hwz1 = 0\n",
    "\n",
    "(count_hwz1, numSentences_hwz1) = checkFirstWordCapitalised(hwz_1)\n",
    "total_count_hwz1 += count_hwz1\n",
    "total_numSentences_hwz1 += numSentences_hwz1\n",
    "\n",
    "print(\"Total count of capitalised first words: \", total_count_hwz1)\n",
    "print(\"Total number of sentences: \", total_numSentences_hwz1)\n",
    "print(\"Percentage: \", (total_count_hwz1/total_numSentences_hwz1) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_count_hwz1 = 0\n",
    "total_pronoun_hwz1 = 0\n",
    "\n",
    "(count_hwz1, numSentences_hwz1) = checkProperNounCapitalised(hwz_1, nlp)    \n",
    "pronoun_count_hwz1 += count_hwz1\n",
    "total_pronoun_hwz1 += numSentences_hwz1\n",
    "\n",
    "print(\"Total count of non-capitalised capitalised pronouns: \", pronoun_count_hwz1)\n",
    "print(\"Total number of pronouns: \", total_pronoun_hwz1)\n",
    "print(\"Percentage: \", (pronoun_count_hwz1/total_pronoun_hwz1) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HardwareZone #2\n",
    "\n",
    "Post Name: Need advice. No internet for desktop on Singtel fibre\n",
    "\n",
    "Website: https://forums.hardwarezone.com.sg/threads/need-advice-no-internet-for-desktop-on-singtel-fibre.6075480/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hwz_2 = scrape_text_hardwarezone('https://forums.hardwarezone.com.sg/threads/need-advice-no-internet-for-desktop-on-singtel-fibre.6075480/')\n",
    "print(hwz_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_language_error_hwz(hwz_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count_hwz2 = 0\n",
    "total_numSentences_hwz2 = 0\n",
    "\n",
    "(count_hwz2, numSentences_hwz2) = checkFirstWordCapitalised(hwz_2)\n",
    "total_count_hwz2 += count_hwz2\n",
    "total_numSentences_hwz2 += numSentences_hwz2\n",
    "\n",
    "print(\"Total count of capitalised first words: \", total_count_hwz2)\n",
    "print(\"Total number of sentences: \", total_numSentences_hwz2)\n",
    "print(\"Percentage: \", (total_count_hwz2/total_numSentences_hwz2) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_count_hwz2 = 0\n",
    "total_pronoun_hwz2 = 0\n",
    "\n",
    "(count_hwz1, numSentences_hwz1) = checkProperNounCapitalised(hwz_2, nlp)    \n",
    "pronoun_count_hwz2 += count_hwz2\n",
    "total_pronoun_hwz2 += numSentences_hwz2\n",
    "\n",
    "print(\"Total count of non-capitalised capitalised pronouns: \", pronoun_count_hwz2)\n",
    "print(\"Total number of pronouns: \", total_pronoun_hwz2)\n",
    "print(\"Percentage: \", (pronoun_count_hwz2/total_pronoun_hwz2) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most frequent ⟨ Noun - Adjective ⟩ pairs for each rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reviewSelected100.json') as f:\n",
    "    data = json.loads(\"[\" + \n",
    "        f.read().replace(\"}\\n{\", \"},\\n{\") + \n",
    "    \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_businesses = set()\n",
    "for review in data:\n",
    "    unique_businesses.add(review['business_id'])\n",
    "print(\"Number of businesses: \" + str(len(unique_businesses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Search Engine using ElasticSearch for Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(HOST=\"http://localhost\", PORT=9200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBC: might use inverted index\n",
    "def extractUniqueReviews(rating, amount, data, unique_businesses):\n",
    "    count = 0\n",
    "    business_set = set()\n",
    "    while (count < amount):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNounAdjectivePairs(review_text, nlp):\n",
    "    results = []\n",
    "    sentences = nltk.tokenize.sent_tokenize(review_text)\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        for i, token in enumerate(doc):\n",
    "            if token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
    "                for j in range(len(doc)):\n",
    "                    if doc[j].pos_ == 'ADJ':\n",
    "                        results.append((token, doc[j]))\n",
    "    return results\n",
    "    \n",
    "s = 'Mark and John are sincere employees at Google. The food is very nice and delicious'\n",
    "print(getNounAdjectivePairs(s, nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLPAssignment1Testing.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "94c38aa216cb19f74293236aacf8c039d75912cab4e786edf53a4f1451d3d752"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
