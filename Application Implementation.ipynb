{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import json\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from random import randrange\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenise all the words with the help of a tokeniser\n",
    "# for model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer() #num_words is the tokeniser that fits the number of words\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D #find out how come they using different types of drop outs\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to use trained model to get sentiment\n",
    "def predict_sentiment(text):#note that 1 denotes positive and 0 denotes negative\n",
    "    model = keras.models.load_model('model')\n",
    "    tw = tokenizer.texts_to_sequences([text])\n",
    "    tw = pad_sequences(tw,maxlen=200)\n",
    "    prediction = int(model.predict(tw).round().item())\n",
    "    if prediction ==1:\n",
    "        print(\"Predicted label: Good sentiment\")\n",
    "    else:\n",
    "       print(\"Predicted label: Bad sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Search\n",
    "\n",
    "Search for reviews based on user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reviewSelected100.json') as f:\n",
    "    data = json.loads(\"[\" + \n",
    "        f.read().replace(\"}\\n{\", \"},\\n{\") + \n",
    "    \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of businesses: 153\n"
     ]
    }
   ],
   "source": [
    "unique_businesses = set()\n",
    "for review in data:\n",
    "    unique_businesses.add(review['business_id'])\n",
    "print(\"Number of businesses: \" + str(len(unique_businesses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(HOST=\"http://localhost\", PORT=9200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define body of index\n",
    "body={\n",
    "    'settings': {\n",
    "        'number_of_shards': 1,\n",
    "        'number_of_replicas': 0,\n",
    "        'index': {\n",
    "          'sort.field': 'date',\n",
    "          'sort.order': 'asc'\n",
    "        },\n",
    "\n",
    "        # custom analyzer\n",
    "        'analysis': {\n",
    "            'analyzer': {\n",
    "                'review_analyzer': {\n",
    "                    'type': 'custom',\n",
    "                      'tokenizer': 'standard',\n",
    "                      'filter': ['lowercase', 'english_stop', 'porter_stem']\n",
    "                    }\n",
    "                  },\n",
    "            'filter': {\n",
    "                'english_stop': { \n",
    "                'type': 'stop',\n",
    "                'stopwords': '_english_'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'text': {\n",
    "                'type': 'text',\n",
    "                'analyzer': 'review_analyzer',\n",
    "                'search_analyzer': 'review_analyzer'\n",
    "            },\n",
    "            'date': {\n",
    "                'type': 'date',\n",
    "                'format': 'yyyy-MM-dd HH:mm:ss'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_generator(data):\n",
    "    for review in data:\n",
    "        yield {\n",
    "                \"_index\": index_name,\n",
    "                \"_type\": \"_doc\",\n",
    "                \"_id\" : f\"{review['review_id']}\",\n",
    "                \"_source\": review,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index already exists\n"
     ]
    }
   ],
   "source": [
    "index_name = \"review-index\"\n",
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(index=index_name, body=body)\n",
    "    helpers.bulk(es, review_generator(data))\n",
    "    print(\"Index created\")\n",
    "else:\n",
    "    print(\"Index already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieveUniqueReviewsByInput(num_of_reviews, userInput, es, index_name):\n",
    "    doc_count = 0\n",
    "    reviews = []\n",
    "    query = {\n",
    "        \"size\": 100,\n",
    "        \"query\": {\n",
    "            \"query_string\": {\n",
    "                \"query\": \"customer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    # Make a search request\n",
    "    res = es.search(index=index_name, body=query, scroll='2s')\n",
    "    \n",
    "    for doc in res['hits']['hits']:\n",
    "        print(\"\\n\", doc['_id'], doc['_source']['business_id'], doc['_source']['text'], doc['_source']['date'], doc['_score'])\n",
    "        doc_count += 1\n",
    "        print(\"DOC COUNT:\", doc_count)\n",
    "        reviews.append(doc['_source'])\n",
    "    \n",
    "    old_scroll_id = res['_scroll_id']\n",
    "    \n",
    "    while len(res['hits']['hits']):\n",
    "        res = es.scroll(scroll_id=old_scroll_id, scroll='2s')\n",
    "        if old_scroll_id != res['_scroll_id']:\n",
    "            print(\"New scroll id: \" + res['_scroll_id'])\n",
    "        old_scroll_id = res['_scroll_id']\n",
    "        \n",
    "        print(\"\\nResponse for index:\", index_name)\n",
    "        print(\"Scroll ID:\", res['_scroll_id'])\n",
    "        print(\"Total Hits:\", res['hits']['total']['value'])\n",
    "        \n",
    "        # Iterate over hits for each scroll\n",
    "        for doc in res['hits']['hits']:\n",
    "            print(\"\\n\", doc['_id'], doc['_source']['text'], doc['_source']['date'], doc['_score'])\n",
    "            doc_count += 1\n",
    "            print(\"DOC COUNT:\", doc_count)\n",
    "            reviews.append(doc['_source'])\n",
    "        \n",
    "    print(\"\\nTOTAL DOC COUNT:\", doc_count)\n",
    "    \n",
    "    # From the reviews retrieved, extract X reviews, one from each unique business\n",
    "    unique_reviews = []\n",
    "    business_set = set()\n",
    "    while len(unique_reviews) < num_of_reviews:\n",
    "        # Find a random review\n",
    "        random_number = randrange(doc_count)\n",
    "        # Don't take review if same business already taken\n",
    "        if reviews[random_number]['business_id'] in business_set:\n",
    "            continue\n",
    "        else:\n",
    "            business_set.add(reviews[random_number]['business_id'])\n",
    "            unique_reviews.append(reviews[random_number])\n",
    "    return unique_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the reviews\n",
    "reviews_input = retrieveUniqueReviewsByInput(50, \"customer\", es, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
